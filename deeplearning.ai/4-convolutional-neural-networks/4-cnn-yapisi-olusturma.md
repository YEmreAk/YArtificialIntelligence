# 4 - CNN Yapısı Oluşturma

## Adım Adım CNN Yapısı

> Convolution model - Step by Step by Andrew Ng

Paketlerin tanımlanması \`\`\`py import numpy as np import h5py import matplotlib.pyplot as plt %matplotlib inline plt.rcParams\['figure.figsize'\] = \(5.0, 4.0\) \# set default size of plots plt.rcParams\['image.interpolation'\] = 'nearest'5 plt.rcParams\['image.cmap'\] = 'gray' %load\_ext autoreload %autoreload 2 np.random.seed\(1\) \`\`\`

Zero Padding \`\`\`py def zero\_pad\(X, pad\): """ Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, as illustrated in Figure 1. Argument: X -- python numpy array of shape \(m, n\_H, n\_W, n\_C\) representing a batch of m images pad -- integer, amount of padding around each image on vertical and horizontal dimensions Returns: X\_pad -- padded image of shape \(m, n\_H + 2\*pad, n\_W + 2\*pad, n\_C\) """ \#\#\# START CODE HERE \#\#\# \(≈ 1 line\) X\_pad = np.pad\(X, \(\(0,0\), \(pad,pad\), \(pad,pad\), \(0,0\)\), 'constant', constant\_values = \(0,0\)\) \#\#\# END CODE HERE \#\#\# return X\_pad \`\`\`

Conv için birim adım \`\`\`py def conv\_single\_step\(a\_slice\_prev, W, b\): """ Apply one filter defined by parameters W on a single slice \(a\_slice\_prev\) of the output activation of the previous layer. Arguments: a\_slice\_prev -- slice of input data of shape \(f, f, n\_C\_prev\) W -- Weight parameters contained in a window - matrix of shape \(f, f, n\_C\_prev\) b -- Bias parameters contained in a window - matrix of shape \(1, 1, 1\) Returns: Z -- a scalar value, result of convolving the sliding window \(W, b\) on a slice x of the input data """ \#\#\# START CODE HERE \#\#\# \(≈ 2 lines of code\) \# Element-wise product between a\_slice\_prev and W. Do not add the bias yet. s = np.multiply\(W, a\_slice\_prev\) \# Sum over all entries of the volume s. Z = s.sum\(\) \# Add bias b to Z. Cast b to a float\(\) so that Z results in a scalar value. Z = Z + float\(b\) \#\#\# END CODE HERE \#\#\# return Z \`\`\`

Conv ilerlemesi \`\`\`py def conv\_forward\(A\_prev, W, b, hparameters\): """ Implements the forward propagation for a convolution function Arguments: A\_prev -- output activations of the previous layer, numpy array of shape \(m, n\_H\_prev, n\_W\_prev, n\_C\_prev\) W -- Weights, numpy array of shape \(f, f, n\_C\_prev, n\_C\) b -- Biases, numpy array of shape \(1, 1, 1, n\_C\) hparameters -- python dictionary containing "stride" and "pad" Returns: Z -- conv output, numpy array of shape \(m, n\_H, n\_W, n\_C\) cache -- cache of values needed for the conv\_backward\(\) function """ \#\#\# START CODE HERE \#\#\# \# Retrieve dimensions from A\_prev's shape \(≈1 line\) \(m, n\_H\_prev, n\_W\_prev, n\_C\_prev\) = A\_prev.shape \# Retrieve dimensions from W's shape \(≈1 line\) \(f, f, n\_C\_prev, n\_C\) = W.shape \# Retrieve information from "hparameters" \(≈2 lines\) stride = hparameters\['stride'\] pad = hparameters\['pad'\] \# Compute the dimensions of the CONV output volume using the formula given above. Hint: use int\(\) to floor. \(≈2 lines\) n\_H = int\(\(n\_H\_prev - f + 2 \* pad\) / stride\) + 1 n\_W = int\(\(n\_W\_prev - f + 2 \* pad\) / stride\) + 1 \# Initialize the output volume Z with zeros. \(≈1 line\) Z = np.zeros\(\(m, n\_H, n\_W, n\_C\)\) \# Create A\_prev\_pad by padding A\_prev A\_prev\_pad = zero\_pad\(A\_prev, pad\) for i in range\(m\): \# loop over the batch of training examples a\_prev\_pad = A\_prev\_pad\[i\] \# Select ith training example's padded activation for h in range\(n\_H\): \# loop over vertical axis of the output volume for w in range\(n\_W\): \# loop over horizontal axis of the output volume for c in range\(n\_C\): \# loop over channels \(= \#filters\) of the output volume \# Find the corners of the current "slice" \(≈4 lines\) vert\_start = h \* stride vert\_end = vert\_start + f horiz\_start = w \* stride horiz\_end = horiz\_start + f \# Use the corners to define the \(3D\) slice of a\_prev\_pad \(See Hint above the cell\). \(≈1 line\) a\_slice\_prev = a\_prev\_pad\[vert\_start:vert\_end, horiz\_start:horiz\_end, :\] \# Convolve the \(3D\) slice with the correct filter W and bias b, to get back one output neuron. \(≈1 line\) Z\[i, h, w, c\] = conv\_single\_step\(a\_slice\_prev, W\[...,c\], b\[...,c\]\) \#\#\# END CODE HERE \#\#\# \# Making sure your output shape is correct assert\(Z.shape == \(m, n\_H, n\_W, n\_C\)\) \# Save information in "cache" for the backprop cache = \(A\_prev, W, b, hparameters\) return Z, cache \`\`\`

Pooling ilerlemesi \`\`\`py def pool\_forward\(A\_prev, hparameters, mode = "max"\): """ Implements the forward pass of the pooling layer Arguments: A\_prev -- Input data, numpy array of shape \(m, n\_H\_prev, n\_W\_prev, n\_C\_prev\) hparameters -- python dictionary containing "f" and "stride" mode -- the pooling mode you would like to use, defined as a string \("max" or "average"\) Returns: A -- output of the pool layer, a numpy array of shape \(m, n\_H, n\_W, n\_C\) cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters """ \# Retrieve dimensions from the input shape \(m, n\_H\_prev, n\_W\_prev, n\_C\_prev\) = A\_prev.shape \# Retrieve hyperparameters from "hparameters" f = hparameters\["f"\] stride = hparameters\["stride"\] \# Define the dimensions of the output n\_H = int\(1 + \(n\_H\_prev - f\) / stride\) n\_W = int\(1 + \(n\_W\_prev - f\) / stride\) n\_C = n\_C\_prev \# Initialize output matrix A A = np.zeros\(\(m, n\_H, n\_W, n\_C\)\) \#\#\# START CODE HERE \#\#\# for i in range\(m\): \# loop over the training examples for h in range\(n\_H\): \# loop on the vertical axis of the output volume for w in range\(n\_W\): \# loop on the horizontal axis of the output volume for c in range \(n\_C\): \# loop over the channels of the output volume \# Find the corners of the current "slice" \(≈4 lines\) vert\_start = h \* stride vert\_end = vert\_start + f horiz\_start = w \* stride horiz\_end = horiz\_start + f \# Use the corners to define the current slice on the ith training example of A\_prev, channel c. \(≈1 line\) a\_prev\_slice = A\_prev\[i, vert\_start:vert\_end, horiz\_start:horiz\_end, c\] \# Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean. if mode == "max": A\[i, h, w, c\] = np.max\(a\_prev\_slice\) elif mode == "average": A\[i, h, w, c\] = np.mean\(a\_prev\_slice\) \#\#\# END CODE HERE \#\#\# \# Store the input and hparameters in "cache" for pool\_backward\(\) cache = \(A\_prev, hparameters\) \# Making sure your output shape is correct assert\(A.shape == \(m, n\_H, n\_W, n\_C\)\) return A, cache \`\`\`

